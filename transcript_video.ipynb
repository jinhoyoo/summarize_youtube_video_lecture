{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generate transcript from Youtube video \n",
    "- Summarize Youtube's script by chapter creater configured. \n",
    "  - Create `markdown_note.md` with script and summary.\n",
    "- Use [yt-dlp](https://pypi.org/project/yt-dlp/), [pydub](https://pypi.org/project/pydub/), [OpenAI-Whisper](https://pypi.org/project/openai-whisper/), [langchain](https://github.com/hwchase17/langchain), and [OpenAI](https://github.com/openai/openai-python) package. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube video ID\n",
    "youtube_video_id=\"O8GLjpnW-cM\"\n",
    "\n",
    "# Language of subscription \n",
    "language = \"ko\"\n",
    "\n",
    "\n",
    "# Officially no way to get chapter automatically, \n",
    "# so copy and paste the time stamp and chapter in description of Youtube video.\n",
    "# CAUTION: Timestamp 00:00 must be defined! \n",
    "chapter_part_in_description = \"\"\"\n",
    "00:00 시작\n",
    "05:56 전해철의 대의원제에 대한 견해: 억압과 착취의 메커니즘\n",
    "19:57 문재인 정부에서 꿀 빨던 인간들은... 도덕성을 말하지 마라\n",
    "34:12 철학, 실력, 용기\n",
    "48:15 우리는 정말 대전환을 원하는가? 관존민비 → 민존관비: ‘그놈정신’이 필요한 이유\n",
    "57:21 이재명의 청계광장 연설(2016.10.29.)\n",
    "1:05:39 노무현의 대통령경선 출마 연설 [새천년민주당, 서울 힐튼호텔(2001.12.10.)]\n",
    "1:15:31 억압과 착취의 메커니즘: 구조, 시스템, 프로세스, 사람\n",
    "1:21:34 국가경영의 지배구조(National Governance)\n",
    "1:31:08 부탁의 말씀, 《더 많은 권력을 시민에게》(토마스 베네딕토, 성연숙 옮김)\n",
    "1:36:24 노무현의 철학, 실력, 용기, 그리고 그의 사고력\n",
    "1:39:31 앵글로색슨 모형(억압과 착취의 구조) vs. 게르만 모형(대화와 토론의 구조)\n",
    "1:42:11 대전환의 의미: ‘그놈정신’으로 무장하라\n",
    "1:44:00 무엇이 가장 중요한가?\n",
    "1:44:45 정리\n",
    "\"\"\"\n",
    "\n",
    "hint_to_fix = \\\n",
    "\"\"\"\n",
    "강진국 -> 강진구\n",
    "박대원 -> 박대용\n",
    "도독성 -> 도덕성\n",
    "윤류의식 -> 윤리의식 \n",
    "알궏리를 -> 알권리를\n",
    "어갑하고 착취하도록 -> 억압하고 착취하도록\n",
    "일어쿽 저러고 -> 이러쿵 저러쿵\n",
    "홍의 -> 혼외\n",
    "나라구 -> 나락\n",
    "도 탐사 -> 더탐사\n",
    "개몽 -> 계몽\n",
    "성충죠 -> 성취죠\n",
    "석진 위원장 -> 혁신 위원장\n",
    "성취혜층 모형 -> 성취예측모형\n",
    "강준규 -> 강진구\n",
    "홍의 자식 -> 혼외자식\n",
    "고질범 51이라는 -> 고작 51일이라는\n",
    "총화대에서 -> 청와대에서\n",
    "문소 -> 문서\n",
    "사회 통염 -> 사회통념\n",
    "포난라서 -> 퍼날라서\n",
    "학교 좀 -> 합격점\n",
    "조직설리 -> 조직설계\n",
    "있어 빌리티만 -> 있어빌리티만\n",
    "문제인 -> 문재인\n",
    "내 마음에 벗과 싸워 -> 내 마음에 법과 싸워\n",
    "이제 명예게 -> 이제명에게\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Officially no way to get chapter automatically, \n",
    "# so we need to parse the text in description and set up the dictionary \n",
    "# [ (time_in_sec, chapter_title) ]\n",
    "import re \n",
    "pattern = r'(\\d+(:\\d+){1,2})\\s(.+)'\n",
    "matches = re.findall(pattern, chapter_part_in_description)\n",
    "\n",
    "def time_to_seconds(time):\n",
    "    parts = time.split(':')\n",
    "    seconds = int(parts[-1])\n",
    "    minutes = int(parts[-2]) if len(parts) > 1 else 0\n",
    "    hours = int(parts[-3]) if len(parts) > 2 else 0\n",
    "    return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "chapters = [(time_to_seconds(time), title.strip()) for time, _, title in matches]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up note with chapter and script under each chapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "\n",
    "# Download youtube video and extract audio file. \n",
    "def download(video_id: str) -> str:\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    ydl_opts = {\n",
    "        'format': 'm4a/bestaudio/best',\n",
    "        'paths': {'home': 'audio/'},\n",
    "        'outtmpl': {'default': '%(id)s.%(ext)s'},\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    if os.path.exists(f'audio/{video_id}.mp3'):\n",
    "        return \"\"\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download([video_url])\n",
    "        if error_code != 0:\n",
    "            raise Exception('Failed to download video')\n",
    "\n",
    "    return f'audio/{video_id}.mp3'\n",
    "\n",
    "file_path = download(youtube_video_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split audio file\n",
    "import os\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "\n",
    "if os.path.exists(file_path) or file_path != '':\n",
    "    audio_data = AudioSegment.from_mp3(file_path)\n",
    "    \n",
    "    for i in range( len(chapters) ):\n",
    "\n",
    "        current_time_in_sec, current_title = chapters[i]\n",
    "        next_time_in_sec, next_title = chapters[i + 1] if i + 1 < len(chapters) else (None, None)\n",
    "\n",
    "        current_time_in_ms = current_time_in_sec * 1000\n",
    "        next_time_in_ms = next_time_in_sec * 1000 if next_time_in_sec is not None else 0\n",
    "\n",
    "        if next_time_in_sec:\n",
    "            splitted_audio_data = audio_data[current_time_in_ms:next_time_in_ms]\n",
    "        else:\n",
    "            splitted_audio_data = audio_data[current_time_in_ms:]\n",
    "\n",
    "        splitted_audio_data.export(f'audio/{i}.mp3' , format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the text from audio files.\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import whisper\n",
    "import llm\n",
    "\n",
    "# Setup OpenAI API key \n",
    "load_dotenv()\n",
    "\n",
    "# Prepare the file path for text\n",
    "text_file_folder_path = os.path.join( os.getcwd(), 'text')\n",
    "if not os.path.exists( text_file_folder_path ):\n",
    "    os.makedirs(text_file_folder_path) \n",
    "\n",
    "\n",
    "# You can adjust the model used here. Model choice is typically a tradeoff between accuracy and speed.\n",
    "# All available models are located at https://github.com/openai/whisper/#available-models-and-languages.\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "\n",
    "script_by_chapter = []\n",
    "def transcribe(file_path: str) -> str:\n",
    "    # `fp16` defaults to `True`, which tells the model to attempt to run on GPU.\n",
    "    # For local demonstration purposes, we'll run this on the CPU by setting it to `False`.\n",
    "    transcription = whisper_model.transcribe(file_path, fp16=False)\n",
    "    return transcription['text'] # type: ignore\n",
    "\n",
    "for i in range( len(chapters) ):\n",
    "    current_time_in_sec, current_title = chapters[i]\n",
    "    print( f'{datetime.now()} : {current_title} is transcripting... \\n' )\n",
    "    audio_file_path = os.path.join( os.getcwd(), 'audio', f'{i}.mp3' )\n",
    "\n",
    "    text_file = os.path.join(text_file_folder_path, f'{i}.txt')\n",
    "\n",
    "    if not os.path.exists(text_file):\n",
    "        transcript = transcribe(audio_file_path)\n",
    "        cleaned_transcript = llm.clean_up_sentence_punctuation_and_fix_errors(transcript, \n",
    "                                                                            hint_to_fix, \n",
    "                                                                            chunk_size=400, \n",
    "                                                                            verbose=True)\n",
    "\n",
    "        merged_text = \\\n",
    "    f\"\"\"\n",
    "    {transcript}\n",
    "\n",
    "    --------\n",
    "\n",
    "    {cleaned_transcript}\n",
    "    \"\"\"\n",
    "\n",
    "        chapter_data = { \n",
    "                    \"title\": current_title,\n",
    "                    \"script\": merged_text,\n",
    "                    \"summary\" : \"\"\n",
    "                    }\n",
    "        \n",
    "        script_by_chapter.append(chapter_data)\n",
    "\n",
    "        # Save transcript file\n",
    "\n",
    "        with open( text_file, \"w\") as file:\n",
    "            file.write(chapter_data[\"script\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temporary save data into file \n",
    "import os \n",
    "import json \n",
    "\n",
    "with open( \"temp_script_by_chapter.json\", \"w\") as file:\n",
    "    file.write( json.dumps(script_by_chapter, indent=2, ensure_ascii=False) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
