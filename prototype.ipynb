{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Populate chapters of Youtube video\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube video ID\n",
    "youtube_video_id=\"MZQ6bc6mPAE\"\n",
    "\n",
    "# Language of subscription \n",
    "language = \"ko\"\n",
    "\n",
    "# Recommended parameters for my testing. \n",
    "max_token = 3000\n",
    "model = \"gpt-3.5-turbo\"\n",
    "chunk_size = 900\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Officially no way to get chapter automatically, \n",
    "# so copy and pagese the time stamp and chapter in description of Youtube video. \n",
    "\n",
    "chapter_part_in_description = \"\"\"\n",
    "00:00 시작\n",
    "05:24 해먹을 결심: 탄핵해야 하는 이유\n",
    "10:55 댓글 읽어보기\n",
    "24:27 인사조직론이란 무엇인가?\n",
    "33:52 (게르만 모형) 왜 직무가 중요한가?\n",
    "43:11 추미애의 직무인식\n",
    "49:16 직무의 존재목적, 칸트의 인간관과 경영학적 응용\n",
    "1:04:40 성과책임의 사회적 의미 (참고)직무의 3대 구성요소\n",
    "1:06:05 직무개념의 부재\n",
    "1:07:44 주진우와 양향자\n",
    "1:11:52 추미애의 고백과 진심\n",
    "1:35:09 왜 역량인가?\n",
    "1:35:47 역량의 개념에 대한 이해\n",
    "1:38:05 《성취예측모형》 프레임워크와 역량사전\n",
    "1:41:48 진실한 리더십과 인재평가의 프레임워크\n",
    "1:43:59 DANO 경영플랫폼 운용_리더십이란 무엇인가?\n",
    "1:49:42 추미애에 대한 오해의 프레임과 추미애의 비전은 무엇인가?\n",
    "1:53:21 이재명과 추미애 vs. 이낙연과 김진표\n",
    "1:55:52 푸른 하늘을(김수영 시인, 1960.06.15.)\n",
    "1:58:57 정리\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Officially no way to get chapter automatically, \n",
    "# so we need to parse the text in description and set up the dictionary \n",
    "# [ (time_in_sec, chapter_title) ]\n",
    "import re \n",
    "pattern = r'(\\d+(:\\d+){1,2})\\s(.+)'\n",
    "matches = re.findall(pattern, chapter_part_in_description)\n",
    "\n",
    "def time_to_seconds(time):\n",
    "    parts = time.split(':')\n",
    "    seconds = int(parts[-1])\n",
    "    minutes = int(parts[-2]) if len(parts) > 1 else 0\n",
    "    hours = int(parts[-3]) if len(parts) > 2 else 0\n",
    "    return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "chapters = [(time_to_seconds(time), title.strip()) for time, _, title in matches]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up note with chapter and script under each chapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Populate the script of YouTube video\n",
    "data = YouTubeTranscriptApi.get_transcripts([youtube_video_id], languages=[language])\n",
    "script_data = deque( data[0][youtube_video_id] )\n",
    "\n",
    "\n",
    "# Put the script under each chatpter\n",
    "# [ \n",
    "#   { \n",
    "#    \"title\": current_title,\n",
    "#    \"script\": script_in_chapter\n",
    "#    } ....\n",
    "#  ]\n",
    "\n",
    "script_by_chapter = []\n",
    "\n",
    "script_in_chapter = \"\"\n",
    "for i in range( len(chapters) ):\n",
    "    current_time_in_sec, current_title = chapters[i]\n",
    "    next_time_in_sec, next_title = chapters[i + 1] if i + 1 < len(chapters) else (None, None)\n",
    "\n",
    "    if len(script_data) == 0:\n",
    "        break\n",
    "\n",
    "    s = script_data.popleft()\n",
    "    end_time_of_script_in_sec = int( s['start'] + s['duration'] )\n",
    "\n",
    "    if next_time_in_sec is not None:\n",
    "        \n",
    "        while end_time_of_script_in_sec < next_time_in_sec:\n",
    "            script_in_chapter += s['text']\n",
    "            script_in_chapter += \" \"\n",
    "            s = script_data.popleft()\n",
    "            end_time_of_script_in_sec = int( s['start'] + s['duration'] )\n",
    "\n",
    "        chapter_data = { \n",
    "                        \"title\": current_title,\n",
    "                        \"script\": script_in_chapter\n",
    "                        }\n",
    "        \n",
    "        script_by_chapter.append(chapter_data)\n",
    "        script_in_chapter = \"\"        \n",
    "\n",
    "    else:\n",
    "        script_in_chapter = \"\"\n",
    "\n",
    "        while len(script_data) > 0 :\n",
    "            script_in_chapter += s['text']\n",
    "            script_in_chapter += \" \"\n",
    "            s = script_data.popleft()\n",
    "            end_time_of_script_in_sec = int( s['start'] + s['duration'] )\n",
    "\n",
    "        chapter_data = { \n",
    "                        \"title\": current_title,\n",
    "                        \"script\": script_in_chapter,\n",
    "                        \"summary\" : \"\"\n",
    "                        }\n",
    "        \n",
    "        script_by_chapter.append(chapter_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temporary save data into file \n",
    "import os \n",
    "import json \n",
    "\n",
    "with open( \"temp_script_by_chapter.json\", \"w\") as file:\n",
    "    file.write( json.dumps(script_by_chapter, indent=2, ensure_ascii=False) )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write note by summarizing contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI API key \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def summarize_text_with_gpt3(text, max_token=3000, model=\"gpt-3.5-turbo\"):\n",
    "    prompt = f\"Summarize following text with bulletin points in Korean:\\n{text}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_token\n",
    "    )\n",
    "\n",
    "    corrected_text = response.choices[0].message.content\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Summarize each chapter \n",
    "for c in script_by_chapter:\n",
    "    texts = text_splitter.split_text(c[\"script\"])\n",
    "\n",
    "    title = c[\"title\"]\n",
    "    print( f\"Chapter {title} is in-pro. \")\n",
    "    \n",
    "    summarized_text = \"\"\n",
    "    for t in texts:\n",
    "        time.sleep(0.05) # Avoid the bad request error. \n",
    "        partial_summary = summarize_text_with_gpt3(t, max_token = max_token, model = model)\n",
    "        summarized_text += partial_summary\n",
    "        print( \".\", end=\"\")\n",
    "\n",
    "    c[\"summary\"] = summarized_text\n",
    "    print('\\n')\n",
    "    time.sleep(0.5) # Avoid the bad request error. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish markdown document\n",
    "\n",
    "Find `markdown_note.md`. This is the summarized note for this Youtube video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"script_by_chapter.json\", \"w\") as file:\n",
    "    file.write( json.dumps(script_by_chapter, indent=2, ensure_ascii=False) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_markdown_text = \"\"\n",
    "\n",
    "for c in script_by_chapter:\n",
    "    full_markdown_text += f\"# {c['title']} \\n\\n\"\n",
    "    full_markdown_text += f\"## Summary \\n\"\n",
    "    full_markdown_text += f\"{c['summary']} \\n\\n\"\n",
    "    full_markdown_text += f\"## Script \\n\\n\"\n",
    "    full_markdown_text += f\"{c['script']} \\n\"\n",
    "    full_markdown_text += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write markdown document for note.\n",
    "with open( \"markdown_note.md\", \"w\") as file:\n",
    "    file.write(full_markdown_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
