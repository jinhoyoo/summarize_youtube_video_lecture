{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Summarize Youtube video lecture \n",
    "- Summarize Youtube's script by chapter creater configured. \n",
    "  - Create `markdown_note.md` with script and summary.\n",
    "- Use [youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/), [langchain](https://github.com/hwchase17/langchain), and [OpenAI](https://github.com/openai/openai-python) package. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libav and ffmpeg. \n",
    "! brew install ffmpeg \n",
    "\n",
    "# For linux (aptitude)\n",
    "# apt-get install libav-tools libavcodec-extra ffmpeg\n",
    "\n",
    "# install package \n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube video ID\n",
    "youtube_video_id=\"esr-1s91_9A\"\n",
    "\n",
    "# Language of subscription \n",
    "language = \"ko\"\n",
    "\n",
    "# LLM: Recommended parameters for my testing. \n",
    "max_token = 15000\n",
    "model = \"gpt-3.5-turbo-16k\"\n",
    "chunk_size = 5000\n",
    "chunk_overlap = 30\n",
    "\n",
    "# Officially no way to get chapter automatically, \n",
    "# so copy and paste the time stamp and chapter in description of Youtube video.\n",
    "\n",
    "chapter_part_in_description = \"\"\"\n",
    "1:12 질서자유주의와 사회적 시장경제\n",
    "3:49 실존철학의 메시지\n",
    "6:32 독일 기본법(헌법)과 대한민국 헌법의 정신\n",
    "18:58 우리는 어떻게 살아가고 있는가?\n",
    "23:54 우리는 앞으로 어떻게 살아야 하는가?\n",
    "28:56 피라미드형 계급구조 vs. 네트워크형 수평구조\n",
    "32:56 의사결정은 반드시 합의를 거친다 (consensus)\n",
    "45:54 대중의 지혜와 집단어리석음\n",
    "50:10 ‘게르만 모형’의 공동결정법과 ‘일하는 방식’\n",
    "54:55 아메리칸 드림에서 유러피언 드림으로\n",
    "1:02:03 자기의식과 변증법적 역사발전\n",
    "1:12:09 “우리는 다리를 놓으며 그 다리를 건너야 한다”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Officially no way to get chapter automatically, \n",
    "# so we need to parse the text in description and set up the dictionary \n",
    "# [ (time_in_sec, chapter_title) ]\n",
    "import re \n",
    "pattern = r'(\\d+(:\\d+){1,2})\\s(.+)'\n",
    "matches = re.findall(pattern, chapter_part_in_description)\n",
    "\n",
    "def time_to_seconds(time):\n",
    "    parts = time.split(':')\n",
    "    seconds = int(parts[-1])\n",
    "    minutes = int(parts[-2]) if len(parts) > 1 else 0\n",
    "    hours = int(parts[-3]) if len(parts) > 2 else 0\n",
    "    return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "chapters = [(time_to_seconds(time), title.strip()) for time, _, title in matches]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up note with chapter and script under each chapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "# Download youtube video and extract audio file. \n",
    "def download(video_id: str) -> str:\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    ydl_opts = {\n",
    "        'format': 'm4a/bestaudio/best',\n",
    "        'paths': {'home': 'audio/'},\n",
    "        'outtmpl': {'default': '%(id)s.%(ext)s'},\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "        }]\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download([video_url])\n",
    "        if error_code != 0:\n",
    "            raise Exception('Failed to download video')\n",
    "\n",
    "    return f'audio/{video_id}.mp3'\n",
    "\n",
    "file_path = download(youtube_video_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split audio file\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "\n",
    "audio_data = AudioSegment.from_mp3(file_path)\n",
    "\n",
    "for i in range( len(chapters) ):\n",
    "    current_time_in_sec, current_title = chapters[i]\n",
    "    prev_time_in_sec, prev_title = chapters[i-1] if i>0 else (None, None)\n",
    "\n",
    "    current_time_in_ms = current_time_in_sec * 1000\n",
    "    prev_time_in_ms = prev_time_in_sec * 1000 if prev_time_in_sec is not None else 0\n",
    "\n",
    "\n",
    "    if prev_time_in_sec:\n",
    "        splitted_audio_data = audio_data[prev_time_in_ms:current_time_in_ms]\n",
    "    else:\n",
    "        splitted_audio_data = audio_data[:current_time_in_ms]\n",
    "\n",
    "    splitted_audio_data.export(f'audio/{i}.mp3' , format=\"mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the text from audio files.\n",
    "import os\n",
    "import whisper\n",
    "\n",
    "\n",
    "# You can adjust the model used here. Model choice is typically a tradeoff between accuracy and speed.\n",
    "# All available models are located at https://github.com/openai/whisper/#available-models-and-languages.\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "\n",
    "script_by_chapter = []\n",
    "def transcribe(file_path: str) -> str:\n",
    "    # `fp16` defaults to `True`, which tells the model to attempt to run on GPU.\n",
    "    # For local demonstration purposes, we'll run this on the CPU by setting it to `False`.\n",
    "    transcription = whisper_model.transcribe(file_path, fp16=False)\n",
    "    return transcription['text']\n",
    "\n",
    "for i in range( len(chapters) ):\n",
    "    current_time_in_sec, current_title = chapters[i]\n",
    "    print( f'{current_title} is transcripting... \\n' )\n",
    "    audio_file_path = os.path.join( os.getcwd(), 'audio', f'{i}.mp3' )\n",
    "    transcript = transcribe(audio_file_path)\n",
    "    chapter_data = { \n",
    "                \"title\": current_title,\n",
    "                \"script\": transcript,\n",
    "                \"summary\" : \"\"\n",
    "                }\n",
    "    script_by_chapter.append(chapter_data)\n",
    "\n",
    "    # Save transcript file\n",
    "    text_file_folder_path = os.path.join( os.getcwd(), 'text')\n",
    "    if not os.path.exists( text_file_folder_path ):\n",
    "        os.makedirs(text_file_folder_path) \n",
    "\n",
    "    text_file = os.path.join(text_file_folder_path, f'{i}.txt')\n",
    "    with open( text_file, \"w\") as file:\n",
    "        file.write(transcript)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temporary save data into file \n",
    "import os \n",
    "import json \n",
    "\n",
    "with open( \"temp_script_by_chapter.json\", \"w\") as file:\n",
    "    file.write( json.dumps(script_by_chapter, indent=2, ensure_ascii=False) )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write note by summarizing contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup OpenAI API key \n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def summarize_text_with_gpt3(text, max_token=3000, model=\"gpt-3.5-turbo\", languages=\"ko\"):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        네가 대학생이고 아래 문장을 요약해서 노트를 만든다고 하자. 최대한 저자의 의도와 문장을 살려서 \n",
    "        bulletin point를 붙여서 요약/정리해줘. 무언가 문장에 이상한 단어가 나오면 () 로 표시해줘.\n",
    "        ----------\n",
    "        {text}\n",
    "        \"\"\"\n",
    "    # prompt = f\"Summarize following text with bulletin points in Korean:\\n{text}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_token\n",
    "    )\n",
    "\n",
    "    corrected_text = response.choices[0].message.content\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    "    length_function = len\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Create temporary folder to store note\n",
    "text_file_folder_path = os.path.join( os.getcwd(), 'note')\n",
    "if not os.path.exists( text_file_folder_path ):\n",
    "    os.makedirs(text_file_folder_path) \n",
    "\n",
    "# Summarize each chapter\n",
    "index = 0\n",
    "for c in script_by_chapter:\n",
    "    \n",
    "    # Split script\n",
    "    texts = text_splitter.split_text(c[\"script\"])\n",
    "\n",
    "    # Summarize the text \n",
    "    title = c[\"title\"]\n",
    "    print( f\"Chapter {title} is in-pro. \")\n",
    "    summarized_text = \"\"\n",
    "    for t in texts:\n",
    "        partial_summary = summarize_text_with_gpt3(t, max_token = max_token, model = model)\n",
    "        summarized_text += partial_summary\n",
    "        print( \".\", end=\"\")\n",
    "\n",
    "    c[\"summary\"] = summarized_text\n",
    "\n",
    "    # Save note into file\n",
    "    text_file = os.path.join(text_file_folder_path, f'{index}.txt')\n",
    "    with open( text_file, \"w\") as file:\n",
    "        file.write(summarized_text)\n",
    "    index += 1\n",
    "\n",
    "    print('\\n')\n",
    "    time.sleep(0.5) # Avoid the bad request error. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish markdown document\n",
    "\n",
    "Find `markdown_note.md`. This is the summarized note for this Youtube video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove temporary data\n",
    "os.remove(\"temp_script_by_chapter.json\")\n",
    "\n",
    "# Save chapter data into file \n",
    "with open( \"script_by_chapter.json\", \"w\") as file:\n",
    "    file.write( json.dumps(script_by_chapter, indent=2, ensure_ascii=False) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_markdown_text = \"\"\n",
    "\n",
    "for c in script_by_chapter:\n",
    "    full_markdown_text += f\"# {c['title']} \\n\\n\"\n",
    "    full_markdown_text += f\"## Summary \\n\"\n",
    "    full_markdown_text += f\"{c['summary']} \\n\\n\"\n",
    "    full_markdown_text += f\"## Script \\n\\n\"\n",
    "    full_markdown_text += f\"{c['script']} \\n\"\n",
    "    full_markdown_text += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write markdown document for note.\n",
    "with open( \"markdown_note.md\", \"w\") as file:\n",
    "    file.write(full_markdown_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
